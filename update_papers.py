import os
import yaml
import google.generativeai as genai
import arxiv
from datetime import datetime
import requests
import time

# --- ì„¤ì • ---
CONFIG_FILE = 'config.yml' # ì„¤ì • íŒŒì¼ ê²½ë¡œ
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')

# --- 1. YAML í—¬í¼ í•¨ìˆ˜ (ë™ì¼) ---

def load_yaml(filename):
    if not os.path.exists(filename):
        return None
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"Error loading {filename}: {e}")
        return None

def save_yaml(data, filename):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            yaml.dump(data, f, allow_unicode=True, sort_keys=False)
    except Exception as e:
        print(f"Error saving {filename}: {e}")

# --- 2. ë…¼ë¬¸ í’ˆì§ˆ í•„í„°ë§ í•¨ìˆ˜ [ì‹ ê·œ] ---

def get_author_hindex_from_semantic_scholar(author_name):
    """
    Semantic Scholar APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ìì˜ h-indexë¥¼ ì¡°íšŒ
    """
    try:
        # ì €ì ê²€ìƒ‰
        search_url = "https://api.semanticscholar.org/graph/v1/author/search"
        params = {"query": author_name, "limit": 1}
        
        response = requests.get(search_url, params=params, timeout=5)
        time.sleep(0.1)  # API rate limit ê³ ë ¤
        
        if response.status_code != 200:
            return None
            
        data = response.json()
        if not data.get('data') or len(data['data']) == 0:
            return None
        
        author_id = data['data'][0].get('authorId')
        if not author_id:
            return None
        
        # ì €ì ìƒì„¸ ì •ë³´ ì¡°íšŒ
        author_url = f"https://api.semanticscholar.org/graph/v1/author/{author_id}"
        params = {"fields": "hIndex,name"}
        
        response = requests.get(author_url, params=params, timeout=5)
        time.sleep(0.1)
        
        if response.status_code != 200:
            return None
            
        author_data = response.json()
        return author_data.get('hIndex', 0)
        
    except Exception as e:
        print(f"Error fetching h-index for {author_name}: {e}")
        return None

def check_author_in_list(author_name, author_list):
    """
    ì €ì ì´ë¦„ì´ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸ (ë¶€ë¶„ ë§¤ì¹­ ì§€ì›)
    """
    author_name_lower = author_name.lower()
    for renowned_author in author_list:
        # ì„±(last name)ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        if renowned_author.split()[-1].lower() in author_name_lower:
            return True
    return False

def check_institution_in_list(affiliation, institution_list):
    """
    ì†Œì† ê¸°ê´€ì´ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸ (ë¶€ë¶„ ë§¤ì¹­ ì§€ì›)
    """
    if not affiliation:
        return False
    affiliation_lower = affiliation.lower()
    for institution in institution_list:
        if institution.lower() in affiliation_lower:
            return True
    return False

def check_journal_published(paper, journal_list):
    """
    ë…¼ë¬¸ì´ ì €ëª…í•œ ì €ë„ì— ì¶œíŒë˜ì—ˆëŠ”ì§€ í™•ì¸
    arXiv APIì˜ journal_ref í•„ë“œ ì‚¬ìš©
    """
    journal_ref = getattr(paper, 'journal_ref', None)
    if not journal_ref:
        return False
    
    journal_ref_lower = journal_ref.lower()
    for journal in journal_list:
        if journal.lower() in journal_ref_lower:
            return True
    return False

def calculate_paper_quality_score(paper, filter_config):
    """
    ë…¼ë¬¸ì˜ í’ˆì§ˆ ì ìˆ˜ë¥¼ ê³„ì‚° (0-10ì  ì²™ë„)
    
    ì˜µì…˜ 1: ì €ëª…í•œ ê¸°ê´€ (2ì )
    ì˜µì…˜ 1: ì €ëª…í•œ ì—°êµ¬ì (3ì )
    ì˜µì…˜ 2: ì €ì h-index (3ì )
    ì˜µì…˜ 3: ì €ë„ ì¶œíŒ (3ì )
    """
    score = 0
    details = []
    
    # ì˜µì…˜ 3: ì €ë„ ì¶œíŒ ì²´í¬ (ê°€ì¥ ë¨¼ì € - ë¹ ë¦„)
    prestigious_journals = filter_config.get('prestigious_journals', [])
    if check_journal_published(paper, prestigious_journals):
        journal_score = filter_config.get('journal_published_score', 3)
        score += journal_score
        details.append(f"ì €ë„ ì¶œíŒ (+{journal_score}ì )")
        print(f"  âœ“ ì €ë„ ì¶œíŒ: {paper.journal_ref}")
    
    # ì˜µì…˜ 1: ì €ëª…í•œ ê¸°ê´€ ë° ì—°êµ¬ì ì²´í¬
    prestigious_institutions = filter_config.get('prestigious_institutions', [])
    renowned_authors = filter_config.get('renowned_authors', [])
    
    renowned_author_found = False
    prestigious_institution_found = False
    
    for author in paper.authors[:3]:  # ì²˜ìŒ 3ëª…ì˜ ì €ìë§Œ ì²´í¬ (ì£¼ì €ì ì¤‘ì‹¬)
        author_name = author.name
        
        # ì €ëª…í•œ ì—°êµ¬ì ì²´í¬
        if not renowned_author_found and check_author_in_list(author_name, renowned_authors):
            score += 3
            details.append(f"ì €ëª…í•œ ì—°êµ¬ì: {author_name} (+3ì )")
            print(f"  âœ“ ì €ëª…í•œ ì—°êµ¬ì: {author_name}")
            renowned_author_found = True
        
        # ì €ëª…í•œ ê¸°ê´€ ì²´í¬ (arXivì—ì„œëŠ” ì†Œì† ì •ë³´ê°€ ì œí•œì )
        # ë…¼ë¬¸ì˜ commentë‚˜ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë©´ ì²´í¬
    
    # comment í•„ë“œì—ì„œ ê¸°ê´€ ì •ë³´ í™•ì¸ (ì¼ë¶€ ë…¼ë¬¸ì— í¬í•¨)
    if not prestigious_institution_found:
        comment = getattr(paper, 'comment', '')
        if comment and check_institution_in_list(comment, prestigious_institutions):
            score += 2
            details.append("ì €ëª…í•œ ê¸°ê´€ (+2ì )")
            print(f"  âœ“ ì €ëª…í•œ ê¸°ê´€ ë°œê²¬")
            prestigious_institution_found = True
    
    # ì˜µì…˜ 2: h-index ì²´í¬ (API í˜¸ì¶œì´ í•„ìš”í•˜ë¯€ë¡œ ë§ˆì§€ë§‰ì—)
    min_hindex = filter_config.get('min_author_hindex', 0)
    if min_hindex > 0 and not renowned_author_found:  # ì´ë¯¸ ì €ëª… ì—°êµ¬ìë¡œ ì¸ì •ë°›ì§€ ì•Šì€ ê²½ìš°ë§Œ
        # ì²« ë²ˆì§¸ ì €ìì˜ h-indexë§Œ ì²´í¬ (API í˜¸ì¶œ ìµœì†Œí™”)
        if len(paper.authors) > 0:
            first_author = paper.authors[0].name
            hindex = get_author_hindex_from_semantic_scholar(first_author)
            
            if hindex and hindex >= min_hindex:
                hindex_score = filter_config.get('hindex_score', 3)
                score += hindex_score
                details.append(f"ì €ì h-index: {hindex} (+{hindex_score}ì )")
                print(f"  âœ“ ì €ì h-index: {hindex} (ê¸°ì¤€: {min_hindex})")
    
    return score, details

# --- 3. ì•„ì¹´ì´ë¹™ í•¨ìˆ˜ [ìˆ˜ì •ë¨] ---
# [ìˆ˜ì •] main í•¨ìˆ˜ì—ì„œ íŒŒì¼ ê²½ë¡œë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
def archive_today_paper(today_path, archive_path):
    print("Archiving 'today_papers'...")
    # [ìˆ˜ì •] í•˜ë“œì½”ë”©ëœ TODAY_FILE ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ today_path ì‚¬ìš©
    today_papers = load_yaml(today_path) 
    
    if not today_papers or not isinstance(today_papers, list):
        print(f"'{today_path}' is empty or not a list. Skipping archive.")
        return

    # [ìˆ˜ì •] í•˜ë“œì½”ë”©ëœ ARCHIVE_FILE ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ archive_path ì‚¬ìš©
    archive_papers = load_yaml(archive_path)
    if archive_papers is None:
        archive_papers = []

    existing_ids = {paper.get('paper_id') for paper in archive_papers if paper.get('paper_id')}
    
    archived_count = 0
    for paper in today_papers:
        if paper.get('paper_id') and paper.get('paper_id') not in existing_ids:
            archive_papers.insert(0, paper)
            archived_count += 1
        else:
            print(f"Paper already in archive or has no ID: {paper.get('title')}")
            
    if archived_count > 0:
        # [ìˆ˜ì •] archive_path ì‚¬ìš©
        save_yaml(archive_papers, archive_path)
        print(f"Archived {archived_count} new papers.")

# --- 4. ìƒˆ ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ [ìˆ˜ì •ë¨ + í•„í„°ë§ ì¶”ê°€] ---
# [ìˆ˜ì •] main í•¨ìˆ˜ì—ì„œ í•„ìš”í•œ ì„¤ì •ê°’ë“¤ì„ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
def find_new_papers(archive_path, query, max_fetch, num_target, filter_config=None):
    print(f"Finding {num_target} new papers from arXiv.org...")
    
    # [ìˆ˜ì •] archive_path ì‚¬ìš©
    archive_papers = load_yaml(archive_path) or []
    existing_ids = {paper.get('paper_id') for paper in archive_papers if paper.get('paper_id')}

    new_papers_list = []
    filter_enabled = filter_config and filter_config.get('enabled', False)
    min_score = filter_config.get('min_score', 0) if filter_enabled else 0

    try:
        client = arxiv.Client()
        search = arxiv.Search(
            # [ìˆ˜ì •] ì¸ìë¡œ ë°›ì€ query, max_fetch ì‚¬ìš©
            query = query,
            max_results = max_fetch, 
            sort_by = arxiv.SortCriterion.SubmittedDate,
            sort_order = arxiv.SortOrder.Descending
        )
        results = list(client.results(search))

        for paper in results:
            paper_id = paper.get_short_id() 
            
            if paper_id not in existing_ids:
                # í’ˆì§ˆ í•„í„°ë§ì´ í™œì„±í™”ëœ ê²½ìš°
                if filter_enabled:
                    print(f"\nê²€í†  ì¤‘: {paper.title[:80]}...")
                    score, details = calculate_paper_quality_score(paper, filter_config)
                    
                    if score >= min_score:
                        print(f"âœ… í•©ê²©! ì ìˆ˜: {score}ì  (ê¸°ì¤€: {min_score}ì )")
                        print(f"   ì„¸ë¶€ì‚¬í•­: {', '.join(details)}")
                        new_papers_list.append(paper)
                    else:
                        print(f"âŒ ë¶ˆí•©ê²©: {score}ì  (ê¸°ì¤€: {min_score}ì )")
                        if details:
                            print(f"   ì„¸ë¶€ì‚¬í•­: {', '.join(details)}")
                        continue
                else:
                    # í•„í„°ë§ ë¹„í™œì„±í™” - ëª¨ë“  ë…¼ë¬¸ ìˆ˜ìš©
                    print(f"Found new paper: {paper.title}")
                    new_papers_list.append(paper)
                
                # [ìˆ˜ì •] ì¸ìë¡œ ë°›ì€ num_target ì‚¬ìš©
                if len(new_papers_list) == num_target: 
                    break
        
        if len(new_papers_list) == 0:
            print("No new papers found that meet the quality criteria.")
            return []
            
        print(f"\nâœ… Found {len(new_papers_list)} qualified new papers total.")
        return new_papers_list

    except Exception as e:
        print(f"Error fetching new paper from arXiv: {e}")
        return []

# --- 4. ë…¼ë¬¸ ìš”ì•½ í•¨ìˆ˜ [ìˆ˜ì •ë¨] ---
# [ìˆ˜ì •] main í•¨ìˆ˜ì—ì„œ ëª¨ë¸ ì´ë¦„ì„ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
def summarize_with_gemini(abstract, model_name):
    if not abstract:
        return "<p>ìš”ì•½í•  ì´ˆë¡ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.</p>"
        
    print(f"Summarizing with Gemini (Model: {model_name})...")
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        # [ìˆ˜ì •] í•˜ë“œì½”ë”©ëœ ëª¨ë¸ëª… ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ model_name ì‚¬ìš©
        model = genai.GenerativeModel(model_name) 
        
        # [ì•„ì´ë””ì–´ 3 ì ìš©] í”„ë¡¬í”„íŠ¸ ê°•í™”
        prompt = f"""
        ë‹¹ì‹ ì€ 2ì°¨ì „ì§€ ë° ì¬ë£Œê³µí•™ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ë‹¤ìŒ ë…¼ë¬¸ì˜ ì´ˆë¡(abstract)ì„ ë°›ì•„ì„œ,
        í•µì‹¬ ë‚´ìš©ì„ [ì—°êµ¬ ë°°ê²½], [ì—°êµ¬ ë°©ë²•], [ì£¼ìš” ê²°ê³¼]ë¡œ êµ¬ë¶„í•˜ì—¬
        **HTML ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸(<ul>) í˜•ì‹**ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

        [ì§€ì‹œ ì‚¬í•­]
        1. ê° í•­ëª©ì€ <li> íƒœê·¸ë¡œ ê°ì‹¸ê³ , <strong> íƒœê·¸ë¡œ ì œëª©ì„ ê°•ì¡°í•´ ì£¼ì„¸ìš”.
        2. ë§ˆí¬ë‹¤ìš´(###, **)ì´ë‚˜ LaTeX($...$) ë¬¸ë²•ì„ **ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
        3. ëª¨ë“  LaTeX ë¬¸ë²•, íŠ¹ìˆ˜ ê¸°í˜¸, ìœ„ì²¨ì/ì•„ë˜ì²¨ìëŠ” 
           'LiCoO2', 'alpha-V2O5' ì²˜ëŸ¼ **ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ í’€ì–´ì“°ì„¸ìš”.**
           (ì˜ˆ: LiCoO$_2$ -> LiCoO2, $\\alpha$ -> alpha, $10^{{10}}$ -> 10^10)

        [ì´ˆë¡ ë‚´ìš©]
        {abstract}
        
        [HTML ìš”ì•½]
        <ul>
          <li><strong>ì—°êµ¬ ë°°ê²½:</strong> ...</li>
          <li><strong>ì—°êµ¬ ë°©ë²•:</strong> ...</li>
          <li><strong>ì£¼ìš” ê²°ê³¼:</strong> ...</li>
        </ul>
        """
        
        response = model.generate_content(prompt)
        return response.text.strip()
        
    except Exception as e:
        print(f"Error summarizing with Gemini: {e}")
        return f"<p>Gemini ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}</p>"
        
# --- 5. ë©”ì¸ ì‹¤í–‰ ë¡œì§ [ìˆ˜ì •ë¨] ---
def main():
    if not GEMINI_API_KEY:
        print("Error: GEMINI_API_KEY is not set in GitHub Secrets.")
        return

    # --- [ìˆ˜ì •] ì„¤ì • íŒŒì¼ ë¡œë“œ ---
    config = load_yaml(CONFIG_FILE)
    if not config:
        print(f"Error: {CONFIG_FILE} not found or empty.")
        return

    # ì„¤ì •ê°’ ë³€ìˆ˜ë¡œ ì‚¬ìš©
    settings = config.get('arxiv_settings', {})
    paths = config.get('file_paths', {})
    filter_config = config.get('quality_filter', {})
    
    # [ì•„ì´ë””ì–´ 2 ì ìš©] ê°•í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš©
    SEARCH_KEYWORDS = settings.get('search_query', 'abs:cathode') # ê¸°ë³¸ê°’ ì„¤ì •
    MAX_RESULTS = settings.get('max_results_to_fetch', 30)
    NUM_PAPERS = settings.get('num_papers_to_summarize', 3)
    
    TODAY_FILE = paths.get('today')
    ARCHIVE_FILE = paths.get('archive')
    
    GEMINI_MODEL = config.get('gemini_model', 'gemini-2.5-flash') # ëª¨ë¸ëª… ë¡œë“œ

    # ì„¤ì •ê°’ ë¡œë“œ í™•ì¸
    if not TODAY_FILE or not ARCHIVE_FILE or not SEARCH_KEYWORDS:
        print("Error: Missing critical paths or search_query in config.yml")
        return
    
    # í’ˆì§ˆ í•„í„° ì„¤ì • ì¶œë ¥
    if filter_config.get('enabled', False):
        print("\nğŸ” í’ˆì§ˆ í•„í„°ë§ í™œì„±í™”ë¨")
        print(f"   ìµœì†Œ ì ìˆ˜: {filter_config.get('min_score', 5)}ì ")
        print(f"   ì €ëª… ê¸°ê´€: {len(filter_config.get('prestigious_institutions', []))}ê°œ")
        print(f"   ì €ëª… ì—°êµ¬ì: {len(filter_config.get('renowned_authors', []))}ëª…")
        print(f"   ìµœì†Œ h-index: {filter_config.get('min_author_hindex', 0)}")
        print(f"   ì €ë„ ëª©ë¡: {len(filter_config.get('prestigious_journals', []))}ê°œ\n")
    # --- ì„¤ì • ë¡œë“œ ì™„ë£Œ ---

    # 1. 'ì˜¤ëŠ˜ì˜ ë…¼ë¬¸' -> 'ì´ì „ ë…¼ë¬¸'ìœ¼ë¡œ ì´ë™
    # [ìˆ˜ì •] ì„¤ì • ë³€ìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬
    archive_today_paper(TODAY_FILE, ARCHIVE_FILE)
    
    # 2. ìƒˆ ë…¼ë¬¸ ê²€ìƒ‰
    # [ìˆ˜ì •] ì„¤ì • ë³€ìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬ + í•„í„° ì„¤ì • ì¶”ê°€
    new_papers = find_new_papers(
        archive_path=ARCHIVE_FILE,
        query=SEARCH_KEYWORDS,
        max_fetch=MAX_RESULTS,
        num_target=NUM_PAPERS,
        filter_config=filter_config
    )
    
    today_papers_data_list = []
    
    if not new_papers:
        print("No new papers to update. Clearing today's list.")
    else:
        # 3. 3ê°œì˜ ë…¼ë¬¸ì„ í•˜ë‚˜ì”© ìš”ì•½í•˜ê³  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        for new_paper in new_papers:
            # [ìˆ˜ì •] ì„¤ì • ë³€ìˆ˜(ëª¨ë¸ëª…)ë¥¼ ì¸ìë¡œ ì „ë‹¬
            summary = summarize_with_gemini(new_paper.summary, GEMINI_MODEL)
            
            paper_data = {
                'title': new_paper.title.strip(),
                'authors': ", ".join([author.name for author in new_paper.authors]),
                'date': new_paper.published.strftime('%Y-%m-%d'),
                'paper_id': new_paper.get_short_id(),
                'link': new_paper.entry_id,
                'summary': summary,
                'summary_date': datetime.now().strftime('%Y-%m-%d %H:%M KST')
            }
            today_papers_data_list.append(paper_data)
            print(f"Processed: {paper_data['title']}")

    # 4. 'today_paper.yml' íŒŒì¼ ë®ì–´ì“°ê¸°
    # [ìˆ˜ì •] ì„¤ì • ë³€ìˆ˜(íŒŒì¼ ê²½ë¡œ) ì‚¬ìš©
    save_yaml(today_papers_data_list, TODAY_FILE)
    print(f"Successfully updated '{TODAY_FILE}' with {len(today_papers_data_list)} papers.")
    
if __name__ == "__main__":
    main()